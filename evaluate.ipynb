{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGeTqaXSXbJ_",
    "outputId": "7f79381d-75b1-4b17-b8a0-63eb008b3612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install openai datasets rouge_score -q\n",
    "!pip install evaluate -q\n",
    "!pip install kiwipiepy -q\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooVncLoiXmFY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from resource.config import api_key, llm_finetune\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETShS-5_RJeL",
    "outputId": "652453fe-41e0-43b0-c715-8e496499be2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "BERTScore: {'bert_precision': 55.71163694063822, 'bert_recall': 73.01715413729349, 'bert_f1': 63.05378476778666}\n",
      "Hallucination Score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from kiwipiepy import Kiwi\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# BLEU 및 BERTScore 로드\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Kiwi 객체 전역 선언\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Kiwi 형태소 분석기 기반 토큰화\n",
    "def tokenize_korean(text):\n",
    "    tokens = [token.form for token in kiwi.tokenize(text)]\n",
    "    return tokens\n",
    "\n",
    "# BERTScore 계산\n",
    "def compute_bertscore_metrics(predictions, references):\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"ko\")\n",
    "    return {\n",
    "        'bert_precision': sum(results['precision']) / len(results['precision']) * 100,\n",
    "        'bert_recall': sum(results['recall']) / len(results['recall']) * 100,\n",
    "        'bert_f1': sum(results['f1']) / len(results['f1']) * 100,\n",
    "    }\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model_with_hallucination(model_name, dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    hallucinated_count = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        input_text = item[\"input\"]\n",
    "        ground_truth = item[\"ground_truth\"]\n",
    "\n",
    "        # 모델 호출\n",
    "        generated_text = generate_response2(input_text, model_name)\n",
    "\n",
    "        # 예측 및 참조 추가\n",
    "        predictions.append(generated_text)\n",
    "        references.append(ground_truth)\n",
    "\n",
    "        # Hallucination 평가\n",
    "        if not any(word in \" \".join(ground_truth) for word in generated_text.split()):\n",
    "            hallucinated_count += 1\n",
    "\n",
    "    # 메트릭 계산\n",
    "    bertscore_results = compute_bertscore_metrics(predictions, references)\n",
    "\n",
    "    # Hallucination 점수 계산\n",
    "    hallucination_score = (\n",
    "        hallucinated_count / len(predictions) * 100 if len(predictions) > 0 else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bertscore\": bertscore_results,\n",
    "        \"hallucination_score\": hallucination_score,\n",
    "    }\n",
    "\n",
    "# OpenAI API 설정\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_response2(prompt, model_name):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"응급처치에 대한 전문가로서 내 물음에 대답해줘.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in OpenAI API call: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 예제 데이터셋\n",
    "data = {\n",
    "    \"input\": [\n",
    "        \"화상을 입었을 때 응급처치는 어떻게 하나요?\",\n",
    "        \"코피가 날 때 가장 먼저 해야 할 응급처치는 무엇인가요?\",\n",
    "        \"벌에 쏘였을 때는 어떻게 해야 하나요?\",\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        [\"화상 부위를 흐르는 찬물로 식히세요.\", \"화상 부위에 찬물을 10분 이상 흘려줍니다.\"],\n",
    "        [\"머리를 앞으로 숙이고 코를 손으로 눌러야 합니다.\", \"코피가 나면 머리를 숙이고 코를 압박하세요.\"],\n",
    "        [\"벌침을 제거하고 차가운 수건으로 진정시키세요.\", \"벌침을 신용카드로 긁어 제거합니다.\"],\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# 평가 실행\n",
    "fine_tuned_model = \"gpt-4o-mini-2024-07-18\"\n",
    "results = evaluate_model_with_hallucination(fine_tuned_model, dataset)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"BERTScore: {results['bertscore']}\")\n",
    "print(f\"Hallucination Score: {results['hallucination_score']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNPNGUyfRRD0",
    "outputId": "06b7ac95-6eb4-4bce-c5ba-ec3a23d92c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "BERTScore: {'bert_precision': 69.50087547302246, 'bert_recall': 79.32908336321512, 'bert_f1': 73.76715342203775}\n",
      "Hallucination Score: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# 모델 이름 설정\n",
    "fine_tuned_model = llm_finetune\n",
    "\n",
    "# 평가 실행\n",
    "results = evaluate_model_with_hallucination(fine_tuned_model, dataset)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"BERTScore: {results['bertscore']}\") # 의미론적 유사도 평가\n",
    "print(f\"Hallucination Score: {results['hallucination_score']:.2f}%\") # 허구적 내용 여부 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du93QA3rRyfg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
